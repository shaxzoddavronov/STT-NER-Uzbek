{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T09:57:25.245472Z","iopub.execute_input":"2025-03-05T09:57:25.245764Z","iopub.status.idle":"2025-03-05T09:57:25.250054Z","shell.execute_reply.started":"2025-03-05T09:57:25.245739Z","shell.execute_reply":"2025-03-05T09:57:25.249417Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"#### Firstly, we should install required packages","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --quiet pip\n!pip install --upgrade --quiet datasets[audio]  evaluate jiwer gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T09:57:34.436655Z","iopub.execute_input":"2025-03-05T09:57:34.436985Z","iopub.status.idle":"2025-03-05T09:57:42.092723Z","shell.execute_reply.started":"2025-03-05T09:57:34.436955Z","shell.execute_reply":"2025-03-05T09:57:42.091754Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T09:57:45.223707Z","iopub.execute_input":"2025-03-05T09:57:45.224071Z","iopub.status.idle":"2025-03-05T09:57:48.888358Z","shell.execute_reply.started":"2025-03-05T09:57:45.224039Z","shell.execute_reply":"2025-03-05T09:57:48.887337Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.3)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.5)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"#### Now we need to log in to Hugging Face and define our training arguments","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T09:57:55.737404Z","iopub.execute_input":"2025-03-05T09:57:55.737748Z","iopub.status.idle":"2025-03-05T09:57:56.116483Z","shell.execute_reply.started":"2025-03-05T09:57:55.737716Z","shell.execute_reply":"2025-03-05T09:57:56.115613Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea693b7d477240219cc46d0a10a42f3e"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading and preparing dataset","metadata":{}},{"cell_type":"markdown","source":"#### We will use common voice audio dataset (Version 13.0) to feed into model.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset,DatasetDict\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nimport numpy as np\ndataset_checkpoint='mozilla-foundation/common_voice_13_0'\nlang_attr,task='uz','transcribe'\ndataset=load_dataset(dataset_checkpoint,lang_attr,split='train+validation')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:04:40.922551Z","iopub.execute_input":"2025-03-04T13:04:40.922860Z","iopub.status.idle":"2025-03-04T13:07:12.829188Z","shell.execute_reply.started":"2025-03-04T13:04:40.922835Z","shell.execute_reply":"2025-03-04T13:07:12.828304Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/14.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a831070b822495bb4a092f29c081b0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"common_voice_13_0.py:   0%|          | 0.00/8.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c7dbaca25304d1ba3de28a8a4df3050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"languages.py:   0%|          | 0.00/3.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a21db6005340a5bb947a66af918b82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"release_stats.py:   0%|          | 0.00/65.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a38be5739f114d32bdfd128b951db39a"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for mozilla-foundation/common_voice_13_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_13_0.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"n_shards.json:   0%|          | 0.00/13.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03f9d888c6e7497483d91740e31fa8c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"uz_train_0.tar:   0%|          | 0.00/1.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d69c0dcf7e624b4f8c4be7e41379eafe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"uz_train_1.tar:   0%|          | 0.00/210M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90396c5003f74aaba06963df729433f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"uz_dev_0.tar:   0%|          | 0.00/344M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed4ecfa44aef47d7994a8b5aa7bbbffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"uz_test_0.tar:   0%|          | 0.00/400M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4747d12efa541d0b598563ed08dc49f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"uz_other_0.tar:   0%|          | 0.00/1.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d09650417c94e58be6ba751b44eeaa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"uz_other_1.tar:   0%|          | 0.00/886M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8096b5d4d5af486286d3149cc65ab9ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"uz_other_2.tar:   0%|          | 0.00/969M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07347dd95f504907889f9f902c66b7c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"uz_other_3.tar:   0%|          | 0.00/253M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba497eb3e7e645cf87cd72b638ed6f7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"uz_invalidated_0.tar:   0%|          | 0.00/397M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a9984d70d94dbd8a05b4bd39af1ec3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.tsv:   0%|          | 0.00/11.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfb7eac3bbdd407789fec6bd117123ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev.tsv:   0%|          | 0.00/2.79M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed4f746f832444d0982077327c5eb339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.tsv:   0%|          | 0.00/2.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e1ab6020d334f73b2121c3f075b9a19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"other.tsv:   0%|          | 0.00/29.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a78c7894aca54725b5ba1fd6945633b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"invalidated.tsv:   0%|          | 0.00/3.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c302849da1549f78555dff2b582559b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4b724ed72ee445cb2405ad537192979"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 10141it [00:00, 101393.42it/s]\u001b[A\nReading metadata...: 20281it [00:00, 77760.47it/s] \u001b[A\nReading metadata...: 28389it [00:00, 74950.90it/s]\u001b[A\nReading metadata...: 36043it [00:00, 68708.80it/s]\u001b[A\nReading metadata...: 48286it [00:00, 72405.53it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094c2ba9e3b646da81e187d01d57be9e"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 12061it [00:00, 140079.58it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14b34f58084744f78a61d5701c492aa2"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 12321it [00:00, 160209.13it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating other split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe48165c997b47e682c9346950601c6c"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 16410it [00:00, 164086.86it/s]\u001b[A\nReading metadata...: 32819it [00:00, 163035.66it/s]\u001b[A\nReading metadata...: 49124it [00:00, 148244.25it/s]\u001b[A\nReading metadata...: 64089it [00:00, 144963.61it/s]\u001b[A\nReading metadata...: 78665it [00:00, 142617.87it/s]\u001b[A\nReading metadata...: 92973it [00:00, 136812.22it/s]\u001b[A\nReading metadata...: 106704it [00:00, 134706.63it/s]\u001b[A\nReading metadata...: 127766it [00:00, 137847.07it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating invalidated split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af187085b2e4caba39af195c6030dae"}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 13811it [00:00, 162394.82it/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T12:52:40.181726Z","iopub.execute_input":"2025-03-04T12:52:40.182030Z","iopub.status.idle":"2025-03-04T12:52:40.186873Z","shell.execute_reply.started":"2025-03-04T12:52:40.182006Z","shell.execute_reply":"2025-03-04T12:52:40.186071Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n    num_rows: 60347\n})"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"#### We just need audio and sentence for STT, so we will remove unnecessary features","metadata":{}},{"cell_type":"code","source":"dataset=dataset.remove_columns(['client_id', 'path','up_votes', 'down_votes', 'age',\n                                'gender', 'accent', 'locale', 'segment', 'variant'],)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:08:07.627147Z","iopub.execute_input":"2025-03-04T13:08:07.627531Z","iopub.status.idle":"2025-03-04T13:08:07.639810Z","shell.execute_reply.started":"2025-03-04T13:08:07.627500Z","shell.execute_reply":"2025-03-04T13:08:07.638949Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['audio', 'sentence'],\n    num_rows: 60347\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"markdown","source":"#### Some  preprocessing methods we will apply:\n* #### Undersampling (Most of the available pretrained models have been pretrained on audio datasets at a sampling rate of 16 kHz)\n* #### Removing special characters (Many special characters encoded differently like: /u2019,/u0020 etc.)","metadata":{}},{"cell_type":"code","source":"# ‘ ’\nfrom datasets import Audio\ndataset=dataset.cast_column('audio',Audio(sampling_rate=16000))\n\ndef cleaning_text(samples):\n  text_clened=[sentence.strip() \\\n              .replace('’',\"'\") \\\n               .replace('‘',\"'\")\\\n               .replace('”','\"')\\\n               .replace('“','\"')\\\n               .replace('»','\"')\\\n               .replace('«','\"')\\\n               .replace('`',\"'\")\\\n               \n                for sentence in samples['sentence']]\n  samples['sentence']=text_clened\n  return samples\n    \ncleaned_dataset=dataset.map(cleaning_text,batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:08:16.314876Z","iopub.execute_input":"2025-03-04T13:08:16.315207Z","iopub.status.idle":"2025-03-04T13:08:16.507666Z","shell.execute_reply.started":"2025-03-04T13:08:16.315178Z","shell.execute_reply":"2025-03-04T13:08:16.506624Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60347 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee83f0c61c9b49968539714f514330aa"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Tokenization and Feature Extraction\n","metadata":{}},{"cell_type":"markdown","source":"* #### Whisper Processor-> that combines both the Feature Extractor and Tokenizer.\n* #### Whisper Feature Extractor-> Converts raw audio (16kHz waveform) into Mel spectrograms (80 frequency bins).\n* #### Whisper Tokenizer-> Converts text into token IDs (for training) and tokens back to text (for transcription output).","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperFeatureExtractor,WhisperTokenizer,WhisperProcessor\nlanguage='uzbek'\ntask='transcribe'\nmodel_checkpoint='openai/whisper-large-v2'\nfeature_extractor=WhisperFeatureExtractor.from_pretrained(model_checkpoint)\ntokenizer=WhisperTokenizer.from_pretrained(model_checkpoint,language=language,task=task)\nprocessor=WhisperProcessor.from_pretrained(model_checkpoint,language=language,task=task)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:08:17.983555Z","iopub.execute_input":"2025-03-04T13:08:17.983844Z","iopub.status.idle":"2025-03-04T13:08:29.713131Z","shell.execute_reply.started":"2025-03-04T13:08:17.983821Z","shell.execute_reply":"2025-03-04T13:08:29.712459Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1b71671ed25485691ebb8f36d415a78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"595b2ac0b10f46cfa750d063e4f64d42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"618222e2aa204e4e992f186d3957e1d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54e2689e1c9e47ebaf22f44e36ade2f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c385f9d7599045c7b46e87c2562efe75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23da1a44010843aca0ae1403d6de5b3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8673417ee8904cc8b582ad13d17dc080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e637b799cd2245acb5ec992f3e9cfb76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3bfbb54268d4bc89f570537c355e0f5"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import os\nnum_of_device=os.cpu_count()\nprint(num_of_device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:08:31.224734Z","iopub.execute_input":"2025-03-04T13:08:31.225205Z","iopub.status.idle":"2025-03-04T13:08:31.229771Z","shell.execute_reply.started":"2025-03-04T13:08:31.225177Z","shell.execute_reply":"2025-03-04T13:08:31.228854Z"}},"outputs":[{"name":"stdout","text":"4\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"#### Actually We should not use entire dataset, since it takes so many time and device memory for fine-tuning.\n#### Let's take just 10.000 dataset raws","metadata":{}},{"cell_type":"code","source":"dataset=dataset.select(range(10000))\ndataset=dataset.train_test_split(test_size=0.1,seed=23)\ndataset['validation']=dataset.pop('test')\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:08:32.654911Z","iopub.execute_input":"2025-03-04T13:08:32.655451Z","iopub.status.idle":"2025-03-04T13:08:32.679039Z","shell.execute_reply.started":"2025-03-04T13:08:32.655415Z","shell.execute_reply":"2025-03-04T13:08:32.678296Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['audio', 'sentence'],\n        num_rows: 9000\n    })\n    validation: Dataset({\n        features: ['audio', 'sentence'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"#### As mentioned above, we will extract features","metadata":{}},{"cell_type":"code","source":"def prepare_features(samples):\n  audio_array=samples['audio']['array']\n  sentence=samples['sentence']\n  sampling_rate=samples['audio']['sampling_rate']\n\n  samples['input_features']=feature_extractor(audio_array,sampling_rate=sampling_rate,return_tensors='pt').input_features[0]\n  samples['labels']=tokenizer(sentence).input_ids\n\n  return samples\n\ndataset_prep=dataset.map(prepare_features,remove_columns=dataset.column_names['train'],\n                         num_proc=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:08:38.925128Z","iopub.execute_input":"2025-03-04T13:08:38.925442Z","iopub.status.idle":"2025-03-04T13:12:52.452201Z","shell.execute_reply.started":"2025-03-04T13:08:38.925420Z","shell.execute_reply":"2025-03-04T13:12:52.451019Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ca36aa649ce40719c68504a58421afc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68d584df419a42cda1086830b8656b5c"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"#### Now, we define evaluation metric.\n#### WER (Word Error Rate) is mostly used metric to calculate STT accuracy\n#### How it work:\n<img src=\"https://sonix.ai/packs/media/images/corp/articles/word-error-rate-2017-c5aba7282b39531154f5676a184c7ec4.png\" width=\"500\">","metadata":{},"attachments":{"c3e14300-7f56-4983-8f20-003eb16e6e13.png":{"image/png":"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"}}},{"cell_type":"code","source":"import evaluate\nmetric=evaluate.load('wer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:12:58.595507Z","iopub.execute_input":"2025-03-04T13:12:58.596196Z","iopub.status.idle":"2025-03-04T13:13:01.371762Z","shell.execute_reply.started":"2025-03-04T13:12:58.596164Z","shell.execute_reply":"2025-03-04T13:13:01.370841Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a72985bcd9c4f15913375a317e42ff0"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"#### Also we define data collator function","metadata":{}},{"cell_type":"code","source":"import torch\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:13:05.804075Z","iopub.execute_input":"2025-03-04T13:13:05.804433Z","iopub.status.idle":"2025-03-04T13:13:05.810955Z","shell.execute_reply.started":"2025-03-04T13:13:05.804404Z","shell.execute_reply":"2025-03-04T13:13:05.810240Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:13:08.227633Z","iopub.execute_input":"2025-03-04T13:13:08.227953Z","iopub.status.idle":"2025-03-04T13:13:08.231698Z","shell.execute_reply.started":"2025-03-04T13:13:08.227927Z","shell.execute_reply":"2025-03-04T13:13:08.230750Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Fine-Tuning model","metadata":{}},{"cell_type":"markdown","source":"#### Let's briefly talk about Whsiper\n**The Whisper model, developed by OpenAI, is a speech-to-text (ASR) model that transcribes audio into text. It is a transformer-based model trained on 680,000 hours of multilingual and multitask data, making it highly robust for diverse accents, background noise, and multiple languages.**","metadata":{}},{"cell_type":"markdown","source":" **According to whisper-large-v2 model, it has around 1.5 bn parameters. It  means that fine-tuning process requires so much time and computational resources. But, we can use PEFT (Parameter Efficent Fine-Tuning) approach for working large models efficently.**\n##### **Most used mehod of PEFT is LORA(Low-Rank Adaption:**\n<img src=\"https://miro.medium.com/v2/resize:fit:979/1*-uBCFiiNeOeTLvUtfdA85w.png\" width=\"500\">","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration\nmodel_checkpoint='openai/whisper-large-v2'\nmodel = WhisperForConditionalGeneration.from_pretrained(model_checkpoint,load_in_8bit=True,device_map='auto')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:13:11.962897Z","iopub.execute_input":"2025-03-04T13:13:11.963206Z","iopub.status.idle":"2025-03-04T13:13:47.318525Z","shell.execute_reply.started":"2025-03-04T13:13:11.963182Z","shell.execute_reply":"2025-03-04T13:13:47.317768Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb371a1243ea41a8955c6febbe4e99ce"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"794a22c4ce8344759b8ca5b29ca73253"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/4.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc20d44ba7d04225b5a7a4c4ab94194d"}},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"#### We configure LORA","metadata":{}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\nfrom peft import LoraConfig,LoraModel,PeftModel,get_peft_model\n\nmodel=prepare_model_for_kbit_training(model)\n\ndef make_inputs_require_grad(module, input, output):\n    output.requires_grad_(True)\n\nmodel.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)\n\nconfig=LoraConfig(\n    r=32,\n    lora_alpha=64,\n    target_modules=['q_proj','v_proj'],\n    lora_dropout=0.05,\n    bias='none',\n)\nmodel=get_peft_model(model,config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:13:49.623835Z","iopub.execute_input":"2025-03-04T13:13:49.624170Z","iopub.status.idle":"2025-03-04T13:13:50.593604Z","shell.execute_reply.started":"2025-03-04T13:13:49.624144Z","shell.execute_reply":"2025-03-04T13:13:50.592854Z"}},"outputs":[{"name":"stdout","text":"trainable params: 15,728,640 || all params: 1,559,033,600 || trainable%: 1.0089\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"#### We are ONLY using 1% of the total trainable parameters, thereby performing Parameter-Efficient Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"#### Then we define training arguments argument","metadata":{}},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"ShakhzoDavronov/whisper-large-lora-uz\",\n    report_to='none',\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n    learning_rate=1e-3,\n    warmup_steps=50,\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    fp16=True,\n    per_device_eval_batch_size=4,\n    generation_max_length=128,\n    logging_steps=300,\n#    max_steps=100, # only for testing purposes, remove this from your final run :)\n    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n    label_names=[\"labels\"],  # same reason as above\n)\nimport torch\ntorch.cuda.empty_cache()  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:14:04.075694Z","iopub.execute_input":"2025-03-04T13:14:04.076008Z","iopub.status.idle":"2025-03-04T13:14:04.111110Z","shell.execute_reply.started":"2025-03-04T13:14:04.075981Z","shell.execute_reply":"2025-03-04T13:14:04.110127Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\nfrom transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n\n# This callback helps to save only the adapter weights and remove the base model weights.\nclass SavePeftModelCallback(TrainerCallback):\n    def on_save(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ):\n        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n\n        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n        kwargs[\"model\"].save_pretrained(peft_model_path)\n\n        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n        if os.path.exists(pytorch_model_path):\n            os.remove(pytorch_model_path)\n        return control\n\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=dataset_prep[\"train\"],\n    eval_dataset=dataset_prep[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=processor.feature_extractor,\n    callbacks=[SavePeftModelCallback],\n)\nmodel.config.use_cache=False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:14:09.338062Z","iopub.execute_input":"2025-03-04T13:14:09.338390Z","iopub.status.idle":"2025-03-04T13:14:10.949822Z","shell.execute_reply.started":"2025-03-04T13:14:09.338365Z","shell.execute_reply":"2025-03-04T13:14:10.948981Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-21-f6bbe916fd7d>:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:14:13.762214Z","iopub.execute_input":"2025-03-04T13:14:13.762552Z","iopub.status.idle":"2025-03-04T13:14:13.766392Z","shell.execute_reply.started":"2025-03-04T13:14:13.762528Z","shell.execute_reply":"2025-03-04T13:14:13.765478Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"#### Finally, we will begin fine-tuning process","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:14:16.416224Z","iopub.execute_input":"2025-03-04T13:14:16.416554Z","iopub.status.idle":"2025-03-04T17:59:34.261075Z","shell.execute_reply.started":"2025-03-04T13:14:16.416529Z","shell.execute_reply":"2025-03-04T17:59:34.260348Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1125/1125 4:45:01, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>300</td>\n      <td>1.191500</td>\n      <td>0.819220</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.752200</td>\n      <td>0.639745</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.624600</td>\n      <td>0.550214</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1125, training_loss=0.7942371622721354, metrics={'train_runtime': 17117.3409, 'train_samples_per_second': 0.526, 'train_steps_per_second': 0.066, 'total_flos': 1.93123823616e+19, 'train_loss': 0.7942371622721354, 'epoch': 1.0})"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"#### Pushing fine-tuned model to huggingface","metadata":{}},{"cell_type":"code","source":"peft_model_id = \"ShakhzoDavronov/whisper-large-lora-uz\"\nmodel.push_to_hub(peft_model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T17:59:40.401905Z","iopub.execute_input":"2025-03-04T17:59:40.402247Z","iopub.status.idle":"2025-03-04T17:59:45.702294Z","shell.execute_reply.started":"2025-03-04T17:59:40.402221Z","shell.execute_reply":"2025-03-04T17:59:45.701340Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/63.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10602964cffd4efa9c0754af137373b9"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/ShakhzoDavronov/whisper-large-lora-uz/commit/7d3175bf3950e837d032cc9dee1991fd47db954f', commit_message='Upload model', commit_description='', oid='7d3175bf3950e837d032cc9dee1991fd47db954f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ShakhzoDavronov/whisper-large-lora-uz', endpoint='https://huggingface.co', repo_type='model', repo_id='ShakhzoDavronov/whisper-large-lora-uz'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T17:59:59.592175Z","iopub.execute_input":"2025-03-04T17:59:59.592555Z","iopub.status.idle":"2025-03-04T17:59:59.610564Z","shell.execute_reply.started":"2025-03-04T17:59:59.592528Z","shell.execute_reply":"2025-03-04T17:59:59.609486Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e607f394fccc4c01b44ec2005f0ba13f"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\nfrom transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n\n#peft_model_id = \"ShakhzoDavronov/whisper-stt-lora-uz\" # Use the same model ID as before.\npeft_config = PeftConfig.from_pretrained(peft_model_id)\nmodel = WhisperForConditionalGeneration.from_pretrained(\n    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(model, peft_model_id)\nmodel.config.use_cache = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T18:00:28.291624Z","iopub.execute_input":"2025-03-04T18:00:28.291906Z","iopub.status.idle":"2025-03-04T18:01:03.112365Z","shell.execute_reply.started":"2025-03-04T18:00:28.291884Z","shell.execute_reply":"2025-03-04T18:01:03.111620Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/842 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d093505436d43d48c82d88d0b88f375"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/63.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01a20b7acfa44e56a341624e3269a192"}},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"#### Let's calculate WER with validation data","metadata":{}},{"cell_type":"code","source":"import gc\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom transformers.models.whisper.english_normalizer import BasicTextNormalizer\n\neval_dataloader = DataLoader(dataset_prep[\"validation\"], batch_size=8, collate_fn=data_collator)\nforced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\nnormalizer = BasicTextNormalizer()\n\npredictions = []\nreferences = []\nnormalized_predictions = []\nnormalized_references = []\n\nmodel.eval()\nfor step, batch in enumerate(tqdm(eval_dataloader)):\n    with torch.cuda.amp.autocast():\n        with torch.no_grad():\n            generated_tokens = (\n                model.generate(\n                    input_features=batch[\"input_features\"].to(\"cuda\"),\n                    forced_decoder_ids=forced_decoder_ids,\n                    max_new_tokens=255,\n                )\n                .cpu()\n                .numpy()\n            )\n            labels = batch[\"labels\"].cpu().numpy()\n            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n            predictions.extend(decoded_preds)\n            references.extend(decoded_labels)\n            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n        del generated_tokens, labels, batch\n    gc.collect()\nwer = 100 * metric.compute(predictions=predictions, references=references)\nnormalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\neval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n\nprint(f\"{wer=} and {normalized_wer=}\")\nprint(eval_metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T18:01:03.113403Z","iopub.execute_input":"2025-03-04T18:01:03.113620Z","iopub.status.idle":"2025-03-04T18:40:57.195699Z","shell.execute_reply.started":"2025-03-04T18:01:03.113602Z","shell.execute_reply":"2025-03-04T18:40:57.194938Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/125 [00:00<?, ?it/s]<ipython-input-27-6a2c04a2b42d>:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n100%|██████████| 125/125 [39:53<00:00, 19.15s/it]\n","output_type":"stream"},{"name":"stdout","text":"wer=47.418948693736226 and normalized_wer=33.14993122420908\n{'eval/wer': 47.418948693736226, 'eval/normalized_wer': 33.14993122420908}\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"#### There is two evalutaion values: WER and normalized WER\n#### Normalized WER is a variant of WER that applies text normalization before computing the error rate.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}